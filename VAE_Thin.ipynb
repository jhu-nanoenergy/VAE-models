{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhu-nanoenergy/VAE-models/blob/Thin-Model/VAE_Thin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYcoVNXkJEud"
      },
      "source": [
        "# Model Setup and Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6ODhKbmW33xN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a48e1e7f-b3ff-4102-a44f-f519048362fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 52.7 MB 164 kB/s \n",
            "\u001b[K     |████████████████████████████████| 225 kB 87.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 70.0 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\u001b[0m\n",
            "\u001b[?25hMounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Helpful tutorial / example links\n",
        "# https://github.com/timbmg/VAE-CVAE-MNIST/blob/master/models.py\n",
        "# https://debuggercafe.com/getting-started-with-variational-autoencoder-using-pytorch/\n",
        "\n",
        "#Basic Packages\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io as spio\n",
        "import scipy.stats as stat\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels\n",
        "import os\n",
        "import random\n",
        "import h5py\n",
        "import sys\n",
        "\n",
        "#PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import argparse\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import random_split\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm import tqdm\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import save_image\n",
        "from torch.utils.checkpoint import checkpoint \n",
        "from torch import autograd\n",
        "\n",
        "#Ray Tune for hyperparameters\n",
        "!pip install -q -U ray\n",
        "from functools import partial\n",
        "from ray import tune\n",
        "from ray.tune import CLIReporter\n",
        "from ray.tune.schedulers import ASHAScheduler\n",
        "\n",
        "#Troubleshooting / Memory\n",
        "!pip install -q -U torchinfo\n",
        "from torchinfo import summary\n",
        "import gc\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" #leftover from debugging but generally useful to have for cuda device side assert errors\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#Plots\n",
        "plt.style.use('ggplot')\n",
        "!pip install -q -U seaborn\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1E9Qyv9qf2zS",
        "outputId": "e1192803-a52d-4e4e-cb46-35f6cfa7426c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['absp', 'ext_in', 'height', 'height_bin', 'lambda', 'masks', 'refl', 'size_frac', 'tran']\n"
          ]
        }
      ],
      "source": [
        "# import data from hdf file\n",
        "fname_mask2 ='/content/drive/MyDrive/Thon Group Master Folder/Sreyas/Photonic Crystals/Rockfish Training Data Gen/allData_thin.h5'\n",
        "hdf_file2 = h5py.File(fname_mask2, \"r\")\n",
        "print(list(hdf_file2.keys()))\n",
        "\n",
        "#Useful data sizes\n",
        "tdn=np.shape(hdf_file2[\"absp\"])[0] #total data number\n",
        "hdn=tdn/2 #half data number\n",
        "wavelengths =hdf_file2[\"lambda\"][:]\n",
        "spec_points=np.shape(wavelengths)[1]\n",
        "half_data_num = 500 # amount of data to use from EACH int / ext dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # DEFINE HYPERPARAMETERS # #\n",
        "# for defining data\n",
        "\n",
        "bsize = 3 # batch size, careful about making bigger because can cause cuda error \n",
        "fv = [256, 256, 512, 1024, 2048] #Channels/Convolutions of Images\n",
        "fv_inv = [1024, 512, 256] #Channels/Tranpose Convolutions of Latent Space\n",
        "ks = 3 #kernel size\n",
        "feat_size = 512 #Feature Space Size\n",
        "latent_features = 20 # dimensionality of latent space\n",
        "\n",
        "#Loss Paramaters\n",
        "alpha = 30 #how much to weight MSE loss\n",
        "beta = 1/40 #how much to weight KLD\n",
        "\n",
        "#Training Parameters \n",
        "epochs = 50 # number of epochs to train for\n",
        "lr = 1e-2 # learning rate of SGD optimizer\n",
        "w_d = 1e-5 # weight decay of SGD optimizer"
      ],
      "metadata": {
        "id": "88a_elwnpe6G"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1UWWAQ5-7r3X"
      },
      "outputs": [],
      "source": [
        "#Different Dataloader for thin data\n",
        "class ImageDataset_thin(Dataset):\n",
        "  #hf is the hdf5 file object\n",
        "  #datanum is the number of datapoints from EACH set that will be used in the model\n",
        "    def __init__(self, hf, datanum, transform= transforms.Compose([ transforms.ToTensor(), transforms.ConvertImageDtype(dtype=torch.float)])):\n",
        "        super(Dataset, self).__init__()\n",
        "\n",
        "        #Format\n",
        "        self.masks = np.concatenate((hf['masks'][:datanum], hf['masks'][25000:25000+datanum]), axis=0) # masks\n",
        "        self.absp = torch.from_numpy(np.concatenate((hf['absp'][:datanum], hf['absp'][25000:25000+datanum]), axis=0))\n",
        "        self.refl = torch.from_numpy(np.concatenate((hf['refl'][:datanum], hf['refl'][25000:25000+datanum]), axis=0))       \n",
        "        self.tran = torch.from_numpy(np.concatenate((hf['tran'][:datanum], hf['tran'][25000:25000+datanum]), axis=0))\n",
        "        self.spectra= torch.stack([self.absp, self.refl, self.tran], dim = 1) #Combine\n",
        "        self.heights = torch.from_numpy(np.concatenate((hf['height'],hf['height'][25000:25000+datanum]), axis=0)) # heights\n",
        "        self.height_bin = torch.from_numpy(np.concatenate((hf['height_bin'][:datanum], hf['height_bin'][25000:25000+datanum]), axis=0)) # bins\n",
        "        self.size_frac =  torch.from_numpy(np.concatenate((hf['size_frac'][:datanum], hf['size_frac'][25000:25000+datanum]), axis=0)) #\n",
        "        self.label = torch.from_numpy(np.concatenate((hf['ext_in'][:datanum], hf['ext_in'][25000:25000+datanum]), axis=0))\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.size_frac)\n",
        "\n",
        "    # currently extracting image, spectra and height \n",
        "    def __getitem__(self, idx): \n",
        "        image = self.masks[idx,:,:] # input mask image\n",
        "        spectra = self.spectra[idx,:]\n",
        "        height = self.heights[idx,:]\n",
        "\n",
        "        if self.transform: \n",
        "            image = self.transform(image)\n",
        "             \n",
        "        return image, spectra, height"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JQFoFVg9-vW9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e9a00a3-d602-4cda-d2ed-ea4711058f37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 219])\n",
            "Split Seed is: 42\n"
          ]
        }
      ],
      "source": [
        "# # Data preparation\n",
        "full_dataset2 = ImageDataset_thin(hdf_file2, half_data_num)\n",
        "dummy=full_dataset2[0][1]\n",
        "print(np.shape(dummy))\n",
        "\n",
        "\n",
        "# Define ratios of train, validation and testing data\n",
        "train_size2 = int(0.7 * len(full_dataset2))\n",
        "val_size2 = int(0.2 * len(full_dataset2))\n",
        "test_size2 = int(0.1 * len(full_dataset2))\n",
        "\n",
        "# Use random split with seed\n",
        "split_seed=42;\n",
        "print(\"Split Seed is:\", split_seed)\n",
        "data_train2, data_val2, data_test2 = torch.utils.data.random_split(full_dataset2, [train_size2, val_size2, test_size2], generator=torch.Generator().manual_seed(split_seed))\n",
        "\n",
        "# Split data into random batches, kills last batch\n",
        "train_dataloader2 = DataLoader(data_train2, batch_size = bsize, shuffle=True, drop_last=True)\n",
        "test_dataloader2 = DataLoader(data_test2, batch_size = bsize, drop_last=True)\n",
        "valid_dataloader2 = DataLoader(data_val2, batch_size = bsize, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model (Thin)"
      ],
      "metadata": {
        "id": "A5RkKFgbDUv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Feature Extraction, Prediction, Recognition\n",
        "class Encoder_thin(nn.Module):\n",
        "  def __init__(self, fv, fv_inv, ks, feat_size, spec_points):\n",
        "    super().__init__()\n",
        "\n",
        "    self.fv = fv #Feature V=Vectors\n",
        "    self.fv_inv = fv_inv #Reconstruction Vecotrs\n",
        "    self.ks = ks #Kernel Size\n",
        "    self.feat_size = feat_size #Latent Space Size\n",
        "    self.spec_points = spec_points ## of wavelengths (always 219)\n",
        "\n",
        "    # Feature Extraction Network \n",
        "    self.enc1 = nn.Sequential(\n",
        "            # Conv_1\n",
        "            nn.Conv2d(1, fv[0], kernel_size=ks, stride=1, padding=\"same\"),\n",
        "            nn.BatchNorm2d(fv[0]),\n",
        "            nn.LeakyReLU(negative_slope=0.1, inplace=True),\n",
        "            \n",
        "            # Conv_2 + Pool_1\n",
        "            nn.Conv2d(fv[0], fv[1], kernel_size=ks,  stride=1, padding=\"same\"),\n",
        "            nn.BatchNorm2d(fv[1]),\n",
        "            nn.LeakyReLU(negative_slope=0.1, inplace=True),     \n",
        "            nn.MaxPool2d(2,2),\n",
        "\n",
        "            # Conv_3 + Pool_2\n",
        "            nn.Conv2d(fv[1], fv[2], kernel_size=ks,  stride=1, padding=\"same\"),\n",
        "            nn.BatchNorm2d(fv[2]),\n",
        "            nn.LeakyReLU(negative_slope=0.1, inplace=True),\n",
        "            nn.MaxPool2d(2,2),\n",
        "\n",
        "            # Conv_4 + Pool_3\n",
        "            nn.Conv2d(fv[2], fv[3], kernel_size=ks,  stride=1, padding=\"same\"),\n",
        "            nn.BatchNorm2d(fv[3]),\n",
        "            nn.LeakyReLU(negative_slope=0.1, inplace=True),\n",
        "            nn.MaxPool2d(2,2),\n",
        "        )\n",
        "    \n",
        "    #Feature Extraction Network to feature layer\n",
        "    self.enc2 = nn.Sequential(\n",
        "            nn.Linear(fv[3]*32*32, feat_size), #Reducing to Feature Size\n",
        "            nn.BatchNorm1d(feat_size)\n",
        "        )\n",
        "\n",
        "    # Prediction network -- Reworked for separate spectra\n",
        "    self.pred1 = nn.Sequential( \n",
        "            nn.Linear(feat_size, feat_size),\n",
        "            nn.BatchNorm1d(feat_size),\n",
        "            nn.Linear(feat_size, feat_size),\n",
        "            nn.BatchNorm1d(feat_size),\n",
        "            nn.Linear(feat_size, spec_points*3)\n",
        "        )\n",
        "\n",
        "\n",
        "    # # Recognition network\n",
        "    self.rec1 =  nn.Sequential( nn.Linear(feat_size+3*spec_points, feat_size),\n",
        "                               nn.BatchNorm1d(feat_size),\n",
        "    )\n",
        "    self.fc_mean = nn.Linear(feat_size, latent_features)\n",
        "    self.fc_cov = nn.Linear(feat_size, latent_features)\n",
        "    \n",
        "  def forward(self, x):\n",
        "\n",
        "    # Run Feature Extraction Network\n",
        "    x = x.float()\n",
        "    x = self.enc1(x)\n",
        "    x = x.float()\n",
        "    flat_size = np.prod(x.size()[1:])\n",
        "    x = x.view(-1, flat_size) \n",
        "    x = self.enc2(x)\n",
        "    \n",
        "    # Run Prediction Network\n",
        "    p = torch.sigmoid(self.pred1(x)) # p = predicted spectra\n",
        "\n",
        "    # Run Recognition Network\n",
        "    input_rec = torch.cat((x, p), 1) # Combine condensed geometry features with the predicted spectra    \n",
        "    x = self.rec1(input_rec)\n",
        "    mu = self.fc_mean(x)\n",
        "    log_var = self.fc_cov(x)\n",
        "\n",
        "    return p, mu, log_var"
      ],
      "metadata": {
        "id": "ExDpUX39DYX5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x = torch.randn(bsize, 1, 256, 256)\n",
        "# net = Encoder_thin(fv, fv_inv, ks, feat_size, spec_points)\n",
        "# p, mu , log_var = net(x)\n",
        "\n",
        "# print(p.shape)\n",
        "# print(mu.shape)\n",
        "print(log_var.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-OYBTWcQjIO",
        "outputId": "31ee8f40-a1ac-4d1a-b831-8fc61d43ab23"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 657])\n",
            "torch.Size([3, 20])\n",
            "torch.Size([3, 20])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder_thin(nn.Module):\n",
        "  def __init__(self, fv, fv_inv, ks, feat_size, spec_points):\n",
        "    super().__init__()\n",
        "\n",
        "    self.fv = fv #Feature V=Vectors\n",
        "    self.fv_inv = fv_inv #Reconstruction Vecotrs\n",
        "    self.ks = ks #Kernel Size\n",
        "    self.feat_size = feat_size #Latent Space Size\n",
        "    self.spec_points = spec_points ## of wavelengths (always 219)\n",
        "\n",
        "    self.recon1 = nn.Sequential(\n",
        "          #Fc_4 Fc_5 Fc_6\n",
        "          nn.Linear(latent_features+3*spec_points, feat_size),\n",
        "          nn.BatchNorm1d(feat_size),\n",
        "          nn.Linear(feat_size, feat_size),\n",
        "          nn.BatchNorm1d(feat_size),\n",
        "          nn.Linear(feat_size, 32*32*fv_inv[0]),\n",
        "          nn.BatchNorm1d(32*32*fv_inv[0]),\n",
        "        )\n",
        "\n",
        "    self.recon2 = nn.Sequential(\n",
        "          nn.ConvTranspose2d(fv_inv[0], fv_inv[1], 3, stride=2, padding = 1, output_padding=1),\n",
        "          nn.ConvTranspose2d(fv_inv[1], fv_inv[2], 3, stride=2, padding = 1, output_padding=1),\n",
        "          nn.ConvTranspose2d(fv_inv[2], 1, 3, stride=2, padding = 1, output_padding=1)\n",
        "        )\n",
        "\n",
        "\n",
        "  def forward(self, spectra, latent):\n",
        "    x = torch.cat((spectra, latent), 1)\n",
        "    x = self.recon1(x)\n",
        "    x = x.view(int(torch.numel(x)/32/32/fv_inv[0]), fv_inv[0], 32, 32)\n",
        "    x = self.recon2(x)\n",
        "    reconstruction =  torch.sigmoid(x)\n",
        "    return reconstruction"
      ],
      "metadata": {
        "id": "fVnT-N6SLS6a"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# spectra = torch.randn(bsize, 3*spec_points)\n",
        "# latent = torch.randn(bsize, 20)\n",
        "\n",
        "# net = Decoder_thin(fv, fv_inv, ks, feat_size, spec_points)\n",
        "# x = net(spectra, latent)\n",
        "# print(x.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1V3XmirEdEv",
        "outputId": "464d40c9-b02d-4d7b-cb2d-cca87abb02b9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 1, 256, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomVAE_thin(nn.Module):\n",
        "    def __init__(self, fv, fv_inv, ks, feat_size, spec_points):\n",
        "        super(CustomVAE_thin, self).__init__()\n",
        "        \n",
        "        self.encoder = Encoder_thin(fv, fv_inv, ks, feat_size, spec_points)\n",
        "        self.decoder = Decoder_thin(fv, fv_inv, ks, feat_size, spec_points)\n",
        "\n",
        "    def reparameterize(self, mu, log_var):\n",
        "        \"\"\"\n",
        "        :param mu: mean from the encoder's latent space\n",
        "        :param log_var: log variance from the encoder's latent space\n",
        "        \"\"\"\n",
        "        std = torch.exp(0.5*log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        sample = mu + (eps * std) # sampling as if coming from the input space\n",
        "        return sample\n",
        "\n",
        "    def forward(self, x):\n",
        "        #Encode       \n",
        "        spectra, mu, log_var = self.encoder(x)\n",
        "\n",
        "        #Reparameterize\n",
        "        sample = self.reparameterize(mu, log_var)\n",
        "\n",
        "        #Reconstruct\n",
        "        recon_x = self.decoder(spectra, sample)\n",
        "        #recon_x = recon_x.view(int(torch.numel(recon_x)/256/256), 256, 256) #Will see what this\n",
        "        return recon_x, mu, log_var, spectra "
      ],
      "metadata": {
        "id": "JUvez4x4R28Z"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x = torch.randn(bsize, 1, 256, 256)\n",
        "# net = CustomVAE_thin(fv, fv_inv, ks, feat_size, spec_points)\n",
        "# recon_x, mu, log_var, spectra = net(x)\n",
        "\n",
        "# print(recon_x.shape)\n",
        "# print(mu.shape)\n",
        "# print(log_var.shape)\n",
        "# print(spectra.shape)\n",
        "\n",
        "# net = None\n",
        "# del net\n",
        "# gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIO2P6_Rcwgv",
        "outputId": "bbcac4d1-7902-40e8-eaf2-3d01e2dc0d14"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 1, 256, 256])\n",
            "torch.Size([3, 20])\n",
            "torch.Size([3, 20])\n",
            "torch.Size([3, 657])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss, Fit, Validation (Thin)"
      ],
      "metadata": {
        "id": "FMKN590hgssi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Loss Paramaters (REPEAT)\n",
        "alpha = 2 #how much to weight MSE loss\n",
        "beta = 1/40 #how much to weight KLD\n",
        "\n",
        "#Training Parameters \n",
        "epochs = 10 # number of epochs to train for\n",
        "lr = 1e-2 # learning rate of SGD optimizer\n",
        "w_d = 1e-5 # weight decay of SGD optimizer"
      ],
      "metadata": {
        "id": "gtKpVYpFdej5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combined loss function that guides the entire network's training\n",
        "def final_loss(loss1_bce, loss2_mse, mu, log_var):\n",
        "    # mu: the mean from the latent vector\n",
        "    # logvar: log variance from the latent vector\n",
        "    \n",
        "    #KLD From:\n",
        "    #https://stats.stackexchange.com/questions/318748/deriving-the-kl-divergence-loss-for-vaes/370048#370048\n",
        "    #https://www.geeksforgeeks.org/role-of-kl-divergence-in-variational-autoencoders/\n",
        "    KLD = 0.5*torch.mean(-1*(log_var+1) + mu.pow(2) - log_var.exp())\n",
        "\n",
        "    return (loss1_bce + alpha*loss2_mse + KLD)"
      ],
      "metadata": {
        "id": "rDj1TClkg0Ax"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fit Function\n",
        "def fit(model, dataloader):\n",
        "    model.train() \n",
        "    running_loss = 0.0\n",
        "    bce_losses = 0.0\n",
        "    mse_losses = 0.0\n",
        "    for i, data in tqdm(enumerate(dataloader), total=int(len(train_dataloader2)/dataloader.batch_size)):\n",
        "      if torch.cuda.is_available():\n",
        "          with autograd.detect_anomaly(): # uncomment this to debug when you receive \"Cuda: device-side assert error\"  \n",
        "          \n",
        "            data, spectra_in, height = [d.cuda( ) for d in data] # load in data from data loader\n",
        "            #data = data.view(int(torch.numel(data)/256/256),  256, 256) # resize data for loss function later\n",
        "            optimizer.zero_grad() # initialize gradients to zero\n",
        "\n",
        "            reconstruction, mu, logvar, out_spectra = model(data) # run model\n",
        "            pout = out_spectra.view(-1, 3, spec_points)  # reformat spectra for plotting\n",
        "          \n",
        "            # leftover code from when i was trying to debug NAN error, potentially unnecessary\n",
        "            #reconstruction = reconstruction.clamp(0,1) # clamp between 0 and 1\n",
        "            #reconstruction[reconstruction!=reconstruction] = 1 # set NAN values  \n",
        "\n",
        "            # solve for loss\n",
        "            bce_loss = criterion_mask(reconstruction, data) \n",
        "            mse_loss = criterion(spectra_in.float(), pout)      \n",
        "            loss = final_loss(bce_loss, mse_loss, mu, logvar)\n",
        "\n",
        "            # add losses to overall loss \"counters\"\n",
        "            running_loss += loss.item()          \n",
        "            bce_losses += bce_loss.item()\n",
        "            mse_losses += mse_loss.item()\n",
        "\n",
        "            # backpropagate loss and then step the optimizer\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    print(f\"Train BCE Loss: {bce_losses:.4f}\")\n",
        "    print(f\"Train MSE Loss: {mse_losses:.4f}\")\n",
        "    train_loss = running_loss\n",
        "    return train_loss, bce_losses, mse_losses"
      ],
      "metadata": {
        "id": "CplOlcUhgSYg"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Validation Function\n",
        "def validate(model, dataloader, plot_on):\n",
        "  model.eval()\n",
        "  running_loss = 0.0\n",
        "  with torch.no_grad(): # all gradients off because currently just evaluating the model\n",
        "    for i, data in tqdm(enumerate(dataloader), total=int(len(valid_dataloader2)/dataloader.batch_size)):\n",
        "        data, spectra_in, height = [d.cuda( ) for d in data] # load in data from data loader\n",
        "        len_temp = int(torch.numel(data)/256/256) # usually this is batch size, but sometimes last batch is a different size\n",
        "        data = data.view(len_temp,  256, 256) # resize data for loss function later\n",
        "\n",
        "        reconstruction, mu, logvar, out_spectra = model(data) # run model\n",
        "        pout = out_spectra.view(-1, 3, spec_points) #reformat spectra for plotting\n",
        "\n",
        "        # leftover code from when i was trying to debug NAN error, potentially unnecessary\n",
        "        # reconstruction = reconstruction.clamp(0,1) # clamp between 0 and 1\n",
        "        # reconstruction[reconstruction!=reconstruction] = 1 # set NAN values  \n",
        "        \n",
        "        # solve for loss\n",
        "        bce_loss = criterion_mask(reconstruction, data) \n",
        "        mse_loss = criterion(spectra_in.float(), pout) \n",
        "        loss = final_loss(bce_loss, mse_loss, mu, logvar)\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # save and plot the last batch input and output \n",
        "        if plot_on:\n",
        "          if i == len_temp - 1:\n",
        "            # first plot shows input geometry vs output geometry\n",
        "            num_replicas = 4\n",
        "            fig, axs = plt.subplots(2,num_replicas)\n",
        "            for x in range( num_replicas ):      \n",
        "              axs[0,x].imshow(torch.squeeze(data.view(len_temp, 1, 256, 256)[x]).cpu())\n",
        "              axs[0,x].xaxis.set_visible(False)\n",
        "              axs[0,x].yaxis.set_visible(False)\n",
        "              axs[1,x].imshow(torch.squeeze(reconstruction.view(len_temp, 1, 256, 256)[x]).cpu())\n",
        "              axs[1,x].xaxis.set_visible(False)\n",
        "              axs[1,x].yaxis.set_visible(False)\n",
        "            fig.suptitle(str(epoch+1))\n",
        "\n",
        "            # second plot shows input spectra vs output spectra\n",
        "            og = spectra_in[0].detach().cpu().numpy()\n",
        "            pred = pout[0].detach().cpu().numpy()\n",
        "            \n",
        "            fig2, axs = plt.subplots(2,1)\n",
        "            axs[0].plot(wavelengths, np.transpose(og) )\n",
        "            axs[1].plot(wavelengths, np.transpose(pred) )\n",
        "            fig2.suptitle(str(epoch+1))\n",
        "\n",
        "            fig.savefig(f\"/content/drive/MyDrive/Thon Group Master Folder/Sreyas/Photonic Crystals/ML Model/VAE/outputs/{epoch+1}geom_output.png\")\n",
        "            fig2.savefig(f\"/content/drive/MyDrive/Thon Group Master Folder/Sreyas/Photonic Crystals/ML Model/VAE/outputs/{epoch+1}spectra_output.png\")\n",
        "            plt.show()\n",
        "            \n",
        "    # val_loss = running_loss/len(dataloader.dataset)\n",
        "    val_loss = running_loss\n",
        "    return val_loss"
      ],
      "metadata": {
        "id": "SXcHQKQjrHSB"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training (Thin)"
      ],
      "metadata": {
        "id": "ZZi66MdmXyAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_custom = CustomVAE_thin(fv, fv_inv, ks, feat_size, spec_points)\n",
        "if torch.cuda.is_available():\n",
        "    model_custom.cuda()\n",
        "\n",
        "#print(summary(model_custom, input_size = (bsize, 1, 256, 256))) # print model summary\n",
        "optimizer = optim.SGD(model_custom.parameters(), lr = lr)\n",
        "criterion_mask = nn.BCELoss(reduction='mean') #BCE\n",
        "criterion = nn.MSELoss(reduction='mean') #MSE"
      ],
      "metadata": {
        "id": "yxoB44L2tUcY"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check that cuda is at acceptable limits\n",
        "torch.cuda.synchronize()\n",
        "torch.cuda.empty_cache()\n",
        "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
      ],
      "metadata": {
        "id": "ixUZhjnJttGf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94a9dc74-cc35-48b1-ef5f-68518baaf90c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    4171 MB |    4171 MB |    4171 MB |       0 B  |\n",
            "|       from large pool |    4168 MB |    4168 MB |    4168 MB |       0 B  |\n",
            "|       from small pool |       3 MB |       3 MB |       3 MB |       0 B  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    4171 MB |    4171 MB |    4171 MB |       0 B  |\n",
            "|       from large pool |    4168 MB |    4168 MB |    4168 MB |       0 B  |\n",
            "|       from small pool |       3 MB |       3 MB |       3 MB |       0 B  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    4176 MB |    4176 MB |    4176 MB |       0 B  |\n",
            "|       from large pool |    4172 MB |    4172 MB |    4172 MB |       0 B  |\n",
            "|       from small pool |       4 MB |       4 MB |       4 MB |       0 B  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    4397 KB |   20209 KB |   37623 KB |   33226 KB |\n",
            "|       from large pool |    3584 KB |   18176 KB |   34560 KB |   30976 KB |\n",
            "|       from small pool |     813 KB |    2039 KB |    3063 KB |    2250 KB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |      89    |      89    |      89    |       0    |\n",
            "|       from large pool |      15    |      15    |      15    |       0    |\n",
            "|       from small pool |      74    |      74    |      74    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |      89    |      89    |      89    |       0    |\n",
            "|       from large pool |      15    |      15    |      15    |       0    |\n",
            "|       from small pool |      74    |      74    |      74    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |       8    |       8    |       8    |       0    |\n",
            "|       from large pool |       6    |       6    |       6    |       0    |\n",
            "|       from small pool |       2    |       2    |       2    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       2    |       3    |       4    |       2    |\n",
            "|       from large pool |       1    |       1    |       2    |       1    |\n",
            "|       from small pool |       1    |       2    |       2    |       1    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Loop over epochs\n",
        "\n",
        "# keep track of different losses\n",
        "train_loss = []\n",
        "val_loss = []\n",
        "train_bce_loss = []\n",
        "train_mse_loss = []\n",
        "train_kld_loss=[]\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1} of {epochs}\")\n",
        "    \n",
        "    # empty cuda cache to help prevent unneeded memory usage\n",
        "    torch.cuda.synchronize()\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    # fit data\n",
        "    train_epoch_loss, train_epoch_bce, train_epoch_mse = fit(model_custom, train_dataloader2) \n",
        "\n",
        "    # test on validation data\n",
        "    if epoch == 0 or not ((epoch+1) % 5): # plot output every 5 epochs\n",
        "      val_epoch_loss = validate(model_custom, valid_dataloader2, 1)\n",
        "    else: # determine validation loss without plotting\n",
        "      val_epoch_loss = validate(model_custom, valid_dataloader2, 0)\n",
        "    \n",
        "    # add to variables\n",
        "    train_loss.append(train_epoch_loss)\n",
        "    train_bce_loss.append(train_epoch_bce)\n",
        "    train_mse_loss.append(alpha*train_epoch_mse)\n",
        "    train_kld_loss.append((train_epoch_loss - train_epoch_bce-alpha*train_epoch_mse))\n",
        "    val_loss.append(val_epoch_loss)\n",
        "\n",
        "    # print progress\n",
        "    print(f\"Train Loss: {train_epoch_loss:.4f}\")\n",
        "    print(f\"Val Loss: {val_epoch_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "qF6Cz3SUtw0-",
        "outputId": "cd9e85fe-09c7-4abe-e5b4-56f5f2b89675"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 of 10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-292b037c9d2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# empty cuda cache to help prevent unneeded memory usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynchronize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36msynchronize\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    491\u001b[0m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_synchronize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot different train losses\n",
        "x = range(len(train_loss))\n",
        "fig_losses, axs = plt.subplots(2,2)\n",
        "axs[0,0].plot(x, train_loss )\n",
        "axs[0,0].set_xlabel(\"Epochs\")\n",
        "axs[0,0].set_ylabel(\"Total\")\n",
        "axs[0,1].plot(x, train_kld_loss)\n",
        "axs[0,1].set_xlabel(\"Epochs\")\n",
        "axs[0,1].set_ylabel(\"beta*KLD\")\n",
        "axs[1,0].plot(x, train_bce_loss)\n",
        "axs[1,0].set_xlabel(\"Epochs\")\n",
        "axs[1,0].set_ylabel(\"BCE\")\n",
        "axs[1,1].plot(x, train_mse_loss)\n",
        "axs[1,1].set_xlabel(\"Epochs\")\n",
        "axs[1,1].set_ylabel(\"alpha*MSE\")\n",
        "\n",
        "fig_losses.suptitle(\"Train Losses\")\n",
        "\n",
        "# save figures\n",
        "fig_losses.savefig(f\"/content/drive/MyDrive/Thon Group Master Folder/Serene/Spectral Selectivity Project/outputs/train_losses_output.png\")"
      ],
      "metadata": {
        "id": "OjswCD-It5Sc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot total validation loss\n",
        "fig_val = plt.figure()\n",
        "plt.plot(x, val_loss)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Combined Validation Loss\")"
      ],
      "metadata": {
        "id": "1Gvgyjm9t7GG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [
        "wYcoVNXkJEud"
      ],
      "machine_shape": "hm",
      "name": "VAE - Thin.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}