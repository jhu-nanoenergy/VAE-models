{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhu-nanoenergy/VAE-models/blob/main/AE_framework_draft.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helpful tutorial / example links\n",
        "# https://github.com/timbmg/VAE-CVAE-MNIST/blob/master/models.py\n",
        "# https://debuggercafe.com/getting-started-with-variational-autoencoder-using-pytorch/"
      ],
      "metadata": {
        "id": "DrTEONV63GMP"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6ODhKbmW33xN"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import scipy.io as spio\n",
        "import scipy.stats as stat\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.optim as optim\n",
        "import argparse\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import save_image\n",
        "from torch.utils.checkpoint import checkpoint \n",
        "import h5py\n",
        "\n",
        "import gc\n",
        "\n",
        "from torch import autograd\n",
        "\n",
        "!pip install -q -U torchinfo\n",
        "from torchinfo import summary\n",
        "\n",
        "plt.style.use('ggplot')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\", force_remount=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMnbMmc2rFYf",
        "outputId": "bbd9445e-fe25-49b4-ca2e-803485fb41ae"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "full_data_num = 500 #amount of data to use from EACH int / ext dataset\n",
        "bsize = 15 #careful making batch size bigger \n",
        "\n",
        "\n",
        "latent_features = 16\n",
        "epochs = 10\n",
        "lr = 1e-2\n",
        "w_d = 1e-5\n",
        "\n",
        "\n",
        "spec_points = 219\n",
        "num_segments = 2\n",
        "\n",
        "# for defining network\n",
        "fv = [256, 256, 512, 1024, 2048] #Channels/Convolutions of Images\n",
        "ks = 3 #kernel size\n",
        "feat_size = 512 #Feature Space Size\n",
        "mv_size = 20 #Latent space sampling size (mean and variance)"
      ],
      "metadata": {
        "id": "G4RlPDkpKAgl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "z7W59gCK_7Vj"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Get all data from drive\n",
        "int_data_all = spio.loadmat('/content/drive/MyDrive/Thon Group Master Folder/Sreyas/Photonic Crystals/Rockfish Training Data Gen/int_total_sqr_no_struct.mat', squeeze_me=True)\n",
        "# ext_data_all = spio.loadmat('/content/drive/MyDrive/Thon Group Master Folder/Sreyas/Photonic Crystals/Rockfish Training Data Gen/ext_total_sqr_no_struct.mat', squeeze_me=True)\n",
        "wavelengths = int_data_all['lambda']\n",
        "wavelengths = np.delete(wavelengths,np.where(wavelengths==[0.5]))\n",
        "wavelengths = np.delete(wavelengths,np.where(wavelengths==[1]))\n",
        "wavelengths = np.transpose(np.tile(wavelengths,(3,1)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.shape(wavelengths))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTa6njqJ2q1K",
        "outputId": "fe10f99b-b2c0-4611-adc7-f95afaaa6802"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(219, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RiUSPvdAFf5N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8d83d36-ebc2-4192-f663-8fbe5c10c778"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ext', 'int']\n",
            "(22, 256, 256)\n"
          ]
        }
      ],
      "source": [
        "# print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
        "fname_mask = '/content/drive/MyDrive/Thon Group Master Folder/Sreyas/Photonic Crystals/Rockfish Training Data Gen/allData.h5'\n",
        "\n",
        "hdf_file = h5py.File(fname_mask, \"r\")\n",
        "print(list(hdf_file.keys()))\n",
        "\n",
        "# dext = hdf_file['ext']\n",
        "dext_spectra = hdf_file['ext/maskCell']\n",
        "dint_spectra = hdf_file['int/maskCell']\n",
        "combined = np.concatenate((dext_spectra[:11],dint_spectra[:11]), axis=0)\n",
        "# print(dext.keys())\n",
        "print(np.shape(combined))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "i2L8E7dWSNBv"
      },
      "outputs": [],
      "source": [
        "class ImageDataset(Dataset):\n",
        "  #hf is the hdf5 file object\n",
        "  #datanum is the number of datapoints from EACH set that will be used in the model\n",
        "    def __init__(self,  hf, datanum, transform= transforms.Compose([ transforms.ToTensor(), transforms.ConvertImageDtype(dtype=torch.float)])  ):\n",
        "        super(Dataset, self).__init__()\n",
        "        dext_height = hf['ext/height']\n",
        "        dint_height = hf['int/height']\n",
        "        dext_spectra = hf['ext/spectCell']\n",
        "        dint_spectra = hf['int/spectCell']\n",
        "        self.spectra = torch.from_numpy(np.concatenate((dext_spectra[:datanum], dint_spectra[:datanum]), axis=0))\n",
        "        \n",
        "        self.heights = torch.from_numpy(np.concatenate((dext_height[:datanum],dint_height[:datanum]), axis=0))\n",
        "        # self.heights = htemp.unsqueeze(1)\n",
        "\n",
        "        self.masks = np.concatenate((hf['ext/maskCell'][:datanum],hf['int/maskCell'][:datanum]), axis=0)\n",
        "        # could use sreyas size calculation instead, not sure\n",
        "        self.sizes =  torch.from_numpy(np.concatenate((hf['ext/size'][:datanum],hf['int/size'])[:datanum], axis=0))\n",
        "        ext_label = np.ones((np.shape(dext_height)))\n",
        "        int_label = np.zeros((np.shape(dint_height)))\n",
        "        self.labels = torch.from_numpy(np.concatenate( (ext_label, int_label), axis=0))\n",
        "        self.transform = transform  \n",
        "              \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.heights)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = (self.masks[idx,:,:])\n",
        "        \n",
        "        # height= np.around( self.heights[idx], decimals=1)\n",
        "        spectra = (( self.spectra[idx] ))\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            \n",
        "\n",
        "        return image, spectra\n",
        "        # return image, spectra, height"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# class ModuleWrapperIgnores2ndArg(nn.Module):\n",
        "#     def __init__(self, module):\n",
        "#         super().__init__()\n",
        "#         self.module = module\n",
        "\n",
        "#     def forward(self,x, dummy_arg=None):\n",
        "#         assert dummy_arg is not None\n",
        "#         x = self.module(x)\n",
        "#         return x"
      ],
      "metadata": {
        "id": "WLuf35s8oR2i"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        " \n",
        "```\n",
        "\n",
        "# Feature Extraction Network inserted into Encoder"
      ],
      "metadata": {
        "id": "u7Rw4snXosYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # encoder, aka Feature Extraction Network\n",
        "    # self.enc1 = nn.Linear(in_features=(256*256), out_features=1024)\n",
        "\n",
        "    self.enc1 = nn.Sequential( # Feature Extraction Network extraction_layers\n",
        "            # Conv_1 \n",
        "            nn.Conv2d(1, fv[0], kernel_size=ks, stride=1, padding=\"same\"),\n",
        "            nn.BatchNorm2d(fv[0]),\n",
        "            nn.LeakyReLU(negative_slope=0.1, inplace=True),\n",
        "            \n",
        "            # Conv_2 + Pool_1\n",
        "            nn.Conv2d(fv[0], fv[1], kernel_size=ks,  stride=1, padding=\"same\"),\n",
        "            nn.BatchNorm2d(fv[1]),\n",
        "            nn.LeakyReLU(negative_slope=0.1, inplace=True),  \n",
        "            nn.MaxPool2d(2,2),\n",
        "\n",
        "            # Conv_3 + Pool_2\n",
        "            nn.Conv2d(fv[1], fv[2], kernel_size=ks,  stride=1, padding=\"same\"),\n",
        "            nn.BatchNorm2d(fv[2]),\n",
        "            nn.LeakyReLU(negative_slope=0.1, inplace=True),\n",
        "            nn.MaxPool2d(2,2),\n",
        "\n",
        "            # Conv_4 + Pool_3\n",
        "            nn.Conv2d(fv[2], fv[3], kernel_size=ks,  stride=1, padding=\"same\"),\n",
        "            nn.BatchNorm2d(fv[3]),\n",
        "            nn.LeakyReLU(negative_slope=0.1, inplace=True),\n",
        "            nn.MaxPool2d(2,2),\n",
        "        )\n",
        "    self.enc2 = nn.Linear(32*32*1024, feat_size) # Feature Extraction Network feature_layer\n",
        "    # self.enc2 = nn.Linear(1024, feat_size) #reducing to 512\n",
        "\n",
        "    # self.dummy_tensor = torch.ones(1, dtype=torch.float, requires_grad = True)    \n",
        "    # self.module_wrapper = ModuleWrapperIgnores2ndArg(self.enc1)\n",
        "    # prediction network\n",
        "    self.pred1 = nn.Linear(in_features=512, out_features=(spec_points*3))\n",
        "\n",
        "    # recognition network\n",
        "    self.rec1 = nn.Linear(in_features=512 + (spec_points*3), out_features=latent_features*2)\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    # encoding\n",
        "    # e = F.relu(self.enc1(x))\n",
        "    \n",
        "    # x = (x.view(bsize, 1, 256, 256))\n",
        "    x = (x.view(int(torch.numel(x)/256/256), 1, 256, 256)) #usually batch size in first arg\n",
        "    \n",
        "    x = self.enc1(x)\n",
        "    # x = checkpoint(self.module_wrapper,x,self.dummy_tensor)\n",
        "\n",
        "    e = x.view(int(torch.numel(x)/32/32/1024), 32*32*1024) #usually batch size in first arg\n",
        "    e = self.enc2(e)\n",
        "    \n",
        "    p = torch.sigmoid(self.pred1(e))    \n",
        "\n",
        "    input_rec = torch.cat((e,p), 1)\n",
        "\n",
        "    x = torch.clamp( self.rec1(input_rec).view(-1, 2, latent_features) , -100, 100)\n",
        "\n",
        "    # get `mu` and `log_var`\n",
        "    mu = x[:, 0, :] # the first feature values as mean\n",
        "    log_var = x[:, 1, :] # the other feature values as variance\n",
        "\n",
        "\n",
        "    return p, mu, log_var"
      ],
      "metadata": {
        "id": "m2NaHNYXom4C"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # decoder \n",
        "    self.dec1 = nn.Linear(in_features= spec_points*3 +latent_features, out_features=512)\n",
        "    self.dec2 = nn.Linear(in_features=512, out_features=(256*256))\n",
        "\n",
        "\n",
        "  def forward(self, spectra,latent):\n",
        "    # encoding\n",
        "    input_dec = torch.cat((spectra,latent),1)\n",
        "\n",
        "    # decoding\n",
        "    x = F.relu(self.dec1(input_dec))\n",
        "    reconstruction =  torch.sigmoid(self.dec2(x) )\n",
        "    return reconstruction "
      ],
      "metadata": {
        "id": "qb-IS3NdpzEO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "WJKSI3gy9t3Z"
      },
      "outputs": [],
      "source": [
        "# primary VAE module\n",
        "\n",
        "class CustomVAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomVAE, self).__init__()\n",
        "\n",
        "        self.encoder = Encoder()\n",
        "        self.decoder = Decoder()\n",
        "\n",
        "    def reparameterize(self, mu, log_var):\n",
        "        \"\"\"\n",
        "        :param mu: mean from the encoder's latent space\n",
        "        :param log_var: log variance from the encoder's latent space\n",
        "        \"\"\"\n",
        "        std = torch.exp(0.5*log_var) # standard deviation\n",
        "        eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
        "        sample = mu + (eps * std) # sampling as if coming from the input space\n",
        "        return sample\n",
        "\n",
        "    def forward(self, x):\n",
        "      \n",
        "        spectra, mu, log_var  = self.encoder(x)        \n",
        "        # get the latent vector through reparameterization\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "        recon_x = self.decoder( spectra, z )\n",
        "        return recon_x, mu, log_var, spectra\n",
        "\n",
        "    # DEFINE SPECTRA PREDICTION FUNCTION \n",
        "\n",
        "\n",
        "    def inference(self, spectra, z):\n",
        "      recon_x = self.decoder( spectra, z )\n",
        "      return recon_x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "tW0UOB4UGbxz"
      },
      "outputs": [],
      "source": [
        "# DEFINE RANDOM SEED \n",
        "# CREATE VALIDATION SET AND FIX MEMORY ISSUES AHHHHHHHHHHH\n",
        "# print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
        "full_dataset = ImageDataset(hdf_file, full_data_num )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMUNitzHsbpu",
        "outputId": "3762cdc0-4932-4989-de83-2492517a1535"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47\n",
            "7\n",
            "14\n",
            "1000\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
        "train_size = int(0.7 * len(full_dataset))\n",
        "val_size = int(0.2 * len(full_dataset))\n",
        "test_size = int(0.1 * len(full_dataset))\n",
        "data_temp, data_test = torch.utils.data.random_split(full_dataset, [train_size+val_size, test_size])\n",
        "\n",
        "data_train, data_val = torch.utils.data.random_split(data_temp, [train_size, val_size])\n",
        "\n",
        "train_dataloader = DataLoader(data_train, batch_size = bsize, shuffle=True)\n",
        "test_dataloader = DataLoader(data_test, batch_size = bsize)\n",
        "valid_dataloader = DataLoader(data_val, batch_size = bsize)\n",
        "print(len(train_dataloader))\n",
        "print(len(test_dataloader))\n",
        "print(len(valid_dataloader))\n",
        "print(len(full_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.cuda.empty_cache()\n",
        "\n",
        "del hdf_file, int_data_all, data_temp\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "ur8oPyVgd2UE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf96d968-2aca-4857-b572-7d663cc95eaf"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "250"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "9azelGQ_RpvB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7a9504b-8959-49f5-82b9-79afb9232ca6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========================================================================================\n",
            "Layer (type:depth-idx)                   Output Shape              Param #\n",
            "==========================================================================================\n",
            "CustomVAE                                --                        --\n",
            "├─Encoder: 1-1                           [15, 657]                 --\n",
            "│    └─Sequential: 2-1                   [15, 1024, 32, 32]        --\n",
            "│    │    └─Conv2d: 3-1                  [15, 256, 256, 256]       2,560\n",
            "│    │    └─BatchNorm2d: 3-2             [15, 256, 256, 256]       512\n",
            "│    │    └─LeakyReLU: 3-3               [15, 256, 256, 256]       --\n",
            "│    │    └─Conv2d: 3-4                  [15, 256, 256, 256]       590,080\n",
            "│    │    └─BatchNorm2d: 3-5             [15, 256, 256, 256]       512\n",
            "│    │    └─LeakyReLU: 3-6               [15, 256, 256, 256]       --\n",
            "│    │    └─MaxPool2d: 3-7               [15, 256, 128, 128]       --\n",
            "│    │    └─Conv2d: 3-8                  [15, 512, 128, 128]       1,180,160\n",
            "│    │    └─BatchNorm2d: 3-9             [15, 512, 128, 128]       1,024\n",
            "│    │    └─LeakyReLU: 3-10              [15, 512, 128, 128]       --\n",
            "│    │    └─MaxPool2d: 3-11              [15, 512, 64, 64]         --\n",
            "│    │    └─Conv2d: 3-12                 [15, 1024, 64, 64]        4,719,616\n",
            "│    │    └─BatchNorm2d: 3-13            [15, 1024, 64, 64]        2,048\n",
            "│    │    └─LeakyReLU: 3-14              [15, 1024, 64, 64]        --\n",
            "│    │    └─MaxPool2d: 3-15              [15, 1024, 32, 32]        --\n",
            "│    └─Linear: 2-2                       [15, 512]                 536,871,424\n",
            "│    └─Linear: 2-3                       [15, 657]                 337,041\n",
            "│    └─Linear: 2-4                       [15, 32]                  37,440\n",
            "├─Decoder: 1-2                           [15, 65536]               --\n",
            "│    └─Linear: 2-5                       [15, 512]                 345,088\n",
            "│    └─Linear: 2-6                       [15, 65536]               33,619,968\n",
            "==========================================================================================\n",
            "Total params: 577,707,473\n",
            "Trainable params: 577,707,473\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (T): 1.17\n",
            "==========================================================================================\n",
            "Input size (MB): 3.93\n",
            "Forward/backward pass size (MB): 11081.03\n",
            "Params size (MB): 2310.83\n",
            "Estimated Total Size (MB): 13395.79\n",
            "==========================================================================================\n"
          ]
        }
      ],
      "source": [
        "# print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
        "model_custom = CustomVAE().cuda()\n",
        "print(summary(model_custom, input_size = (bsize,1,256,256)))\n",
        "# optimizer = optim.Adam(model_custom.parameters(), lr=lr)\n",
        "optimizer = optim.SGD(model_custom.parameters(), lr=lr, weight_decay = w_d)\n",
        "# print(\"took out bce loss idk!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
        "criterion_mask = nn.BCELoss(reduction='sum')\n",
        "# criterion_mask = nn.MSELoss()\n",
        "criterion = nn.MSELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Ag-qT_d6DluX"
      },
      "outputs": [],
      "source": [
        "def final_loss(loss1_bce, loss2_mse, mu, logvar):\n",
        "    \"\"\"\n",
        "    This function will add the reconstruction loss and the  KL-Divergence.\n",
        "    KL-Divergence = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
        "    :param mu: the mean from the latent vector\n",
        "    :param logvar: log variance from the latent vector\n",
        "    \"\"\"\n",
        "    alpha = 2\n",
        "    # print(mu)\n",
        "    # print(logvar)\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    # print(KLD)\n",
        "    return (loss1_bce + alpha*loss2_mse + KLD)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "MYLRiGIgMw0a"
      },
      "outputs": [],
      "source": [
        "def fit(model, dataloader):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in tqdm(enumerate(dataloader), total=int(len(train_dataloader)/dataloader.batch_size)):\n",
        "      if torch.cuda.is_available():\n",
        "        data, spectra_in = [d.cuda( ) for d in data]\n",
        "        data = data.view(data.size(0), -1)\n",
        "        optimizer.zero_grad()\n",
        "        with autograd.detect_anomaly():\n",
        "          reconstruction, mu, logvar, out_spectra = model(data)\n",
        "          pout = out_spectra.view(-1, 3, spec_points)      \n",
        "          \n",
        "          \n",
        "          reconstruction = reconstruction.clamp(0,1) # clamp between 0 and 1\n",
        "          reconstruction[reconstruction!=reconstruction] = 1 # set NAN values  \n",
        "\n",
        "          bce_loss = criterion_mask(reconstruction, data)\n",
        "          \n",
        "          # print(bce_loss)\n",
        "          # print(pout)\n",
        "          mse_loss = criterion(spectra_in.float(), pout)\n",
        "          # print(torch.isfinite(pout).all())\n",
        "          \n",
        "          # print(mse_loss)\n",
        "          loss = final_loss(bce_loss, mse_loss, mu, logvar)\n",
        "          running_loss += loss.item()\n",
        "          # loss.backward(retain_graph=False)\n",
        "          \n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "    train_loss = running_loss/len(dataloader.dataset)\n",
        "    return train_loss\n",
        "    \n",
        "def validate(model, dataloader, plot_on):\n",
        "  model.eval()\n",
        "  running_loss = 0.0\n",
        "  with torch.no_grad():\n",
        "    for i, data in tqdm(enumerate(dataloader), total=int(len(valid_dataloader)/dataloader.batch_size)):\n",
        "        data, spectra_in = [d.cuda( ) for d in data]\n",
        "        data = data.view(data.size(0), -1)\n",
        "        reconstruction, mu, logvar, out_spectra = model(data)\n",
        "        pout = out_spectra.view(-1, 3, spec_points)\n",
        "\n",
        "        reconstruction = reconstruction.clamp(0,1) # clamp between 0 and 1\n",
        "        reconstruction[reconstruction!=reconstruction] = 1 # set NAN values  \n",
        "        \n",
        "        bce_loss = criterion_mask(reconstruction, data)\n",
        "        mse_loss = criterion(spectra_in.float(), pout)\n",
        "        loss = final_loss(bce_loss, mse_loss, mu, logvar)\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # save the last batch input and output \n",
        "        if plot_on:\n",
        "          if i == int(torch.numel(reconstruction)/256/256) - 2:\n",
        "        # if i == 14:   \n",
        "        # if i == 1:\n",
        "            # both = torch.cat((data.view(batch_size, 1, 256, 256)[:4],  \n",
        "            #                   reconstruction.view(batch_size, 1, 256, 256)[:4]))\n",
        "            print(\"MSE loss: \" + str(mse_loss))\n",
        "            num_replicas = 4\n",
        "            fig, axs = plt.subplots(2,num_replicas)\n",
        "            for x in range( num_replicas ):\n",
        "      \n",
        "              axs[0,x].imshow(torch.squeeze(data.view(bsize, 1, 256, 256)[x]).cpu())\n",
        "              axs[0,x].xaxis.set_visible(False)\n",
        "              axs[0,x].yaxis.set_visible(False)\n",
        "              axs[1,x].imshow(torch.squeeze(reconstruction.view(bsize, 1, 256, 256)[x]).cpu())\n",
        "              axs[1,x].xaxis.set_visible(False)\n",
        "              axs[1,x].yaxis.set_visible(False)\n",
        "            fig.suptitle(str(epoch+1))\n",
        "\n",
        "            og = spectra_in[0].detach().cpu().numpy()\n",
        "            pred = pout[0].detach().cpu().numpy()\n",
        "            \n",
        "            # pout = p.view(-1, 3, spec_points)\n",
        "            fig2, axs = plt.subplots(2,1)\n",
        "            axs[0].plot(wavelengths, np.transpose(og) )\n",
        "            axs[1].plot(wavelengths, np.transpose(pred) )\n",
        "            fig2.suptitle(str(epoch+1))\n",
        "            fig.savefig(f\"/content/drive/MyDrive/Thon Group Master Folder/Serene/Spectral Selectivity Project/outputs/{epoch+1}geom_output.png\")\n",
        "            fig2.savefig(f\"/content/drive/MyDrive/Thon Group Master Folder/Serene/Spectral Selectivity Project/outputs/{epoch+1}spectra_output.png\")\n",
        "            plt.show()\n",
        "    val_loss = running_loss/len(dataloader.dataset)\n",
        "    return val_loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.synchronize()\n",
        "torch.cuda.empty_cache()\n",
        "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltNHCfWckbEp",
        "outputId": "48285d68-face-4929-c5f3-b4e04ca3d46c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    2203 MB |    8745 MB |   16055 MB |   13851 MB |\n",
            "|       from large pool |    2203 MB |    8745 MB |   16054 MB |   13851 MB |\n",
            "|       from small pool |       0 MB |       0 MB |       0 MB |       0 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    2203 MB |    8745 MB |   16055 MB |   13851 MB |\n",
            "|       from large pool |    2203 MB |    8745 MB |   16054 MB |   13851 MB |\n",
            "|       from small pool |       0 MB |       0 MB |       0 MB |       0 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |    2216 MB |    8754 MB |    8754 MB |    6538 MB |\n",
            "|       from large pool |    2214 MB |    8752 MB |    8752 MB |    6538 MB |\n",
            "|       from small pool |       2 MB |       2 MB |       2 MB |       0 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |   12495 KB |    3120 MB |    7239 MB |    7227 MB |\n",
            "|       from large pool |   10908 KB |    3118 MB |    7237 MB |    7226 MB |\n",
            "|       from small pool |    1587 KB |       1 MB |       2 MB |       0 MB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |      38    |      46    |      72    |      34    |\n",
            "|       from large pool |       7    |      11    |      27    |      20    |\n",
            "|       from small pool |      31    |      37    |      45    |      14    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |      38    |      46    |      72    |      34    |\n",
            "|       from large pool |       7    |      11    |      27    |      20    |\n",
            "|       from small pool |      31    |      37    |      45    |      14    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |       5    |       8    |       8    |       3    |\n",
            "|       from large pool |       4    |       7    |       7    |       3    |\n",
            "|       from small pool |       1    |       1    |       1    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |       2    |       7    |      15    |      13    |\n",
            "|       from large pool |       1    |       4    |       9    |       8    |\n",
            "|       from small pool |       1    |       5    |       6    |       5    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "id": "AhnrQZhnVF4t",
        "outputId": "83fed5a6-ac35-4cf4-e7ec-a4ac3bf40a5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 of 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/3 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: UserWarning: Anomaly Detection has been enabled. This mode will increase the runtime and should only be enabled for debugging.\n",
            "  if __name__ == '__main__':\n",
            " 33%|███▎      | 1/3 [00:00<00:01,  1.42it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-00e7d4b62a99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# torch.cuda.empty_cache()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrain_epoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_custom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0mval_epoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_custom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-44a2590476de>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, dataloader)\u001b[0m\n\u001b[1;32m     27\u001b[0m           \u001b[0;31m# loss.backward(retain_graph=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m           \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m           \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunning_loss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Function 'AddmmBackward0' returned nan values in its 2th output."
          ]
        }
      ],
      "source": [
        "\n",
        "train_loss = []\n",
        "val_loss = []\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1} of {epochs}\")\n",
        "    # torch.cuda.synchronize()\n",
        "    # torch.cuda.empty_cache()\n",
        "    \n",
        "    train_epoch_loss = fit(model_custom, train_dataloader)\n",
        "    if epoch == 0 or not ((epoch+1) % 5):\n",
        "      val_epoch_loss = validate(model_custom, valid_dataloader, 1)\n",
        "    else:\n",
        "      val_epoch_loss = validate(model_custom, valid_dataloader, 0)\n",
        "    train_loss.append(train_epoch_loss)\n",
        "    val_loss.append(val_epoch_loss)\n",
        "    print(f\"Train Loss: {train_epoch_loss:.4f}\")\n",
        "    print(f\"Val Loss: {val_epoch_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "YjHbx9VYvYtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7kljCxrqckm"
      },
      "outputs": [],
      "source": [
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "As4mNCuKq392"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9U6jxCOke_QU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "AE-framework-draft.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "background_execution": "on",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}