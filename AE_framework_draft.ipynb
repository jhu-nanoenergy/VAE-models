{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhu-nanoenergy/VAE-models/blob/main/AE_framework_draft.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYcoVNXkJEud"
      },
      "source": [
        "# Model Setup and Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DrTEONV63GMP"
      },
      "outputs": [],
      "source": [
        "# Helpful tutorial / example links\n",
        "# https://github.com/timbmg/VAE-CVAE-MNIST/blob/master/models.py\n",
        "# https://debuggercafe.com/getting-started-with-variational-autoencoder-using-pytorch/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ODhKbmW33xN",
        "outputId": "9ff6e917-6e17-4961-9fb4-4480c830c4f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 52.7 MB 1.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 225 kB 79.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 44.3 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import scipy.io as spio\n",
        "import scipy.stats as stat\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import gc\n",
        "import h5py\n",
        "\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" #leftover from debugging but generally useful to have for cuda device side assert errors\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import random_split\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "!pip install -q -U ray\n",
        "from ray import tune\n",
        "from ray.tune import CLIReporter\n",
        "from ray.tune.schedulers import ASHAScheduler\n",
        "import argparse\n",
        "from tqdm import tqdm\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import save_image\n",
        "from torch.utils.checkpoint import checkpoint \n",
        "from torch import autograd\n",
        "\n",
        "!pip install -q -U torchinfo\n",
        "from torchinfo import summary\n",
        "\n",
        "\n",
        "plt.style.use('ggplot')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMnbMmc2rFYf",
        "outputId": "00e25669-aff5-486c-ee74-61ba86ad3ac7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\", force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "G4RlPDkpKAgl"
      },
      "outputs": [],
      "source": [
        "# # DEFINE HYPERPARAMETERS # #\n",
        "# for defining data\n",
        "half_data_num = 500 # amount of data to use from EACH int / ext dataset\n",
        "bsize = 6 # batch size, careful about making bigger because can cause cuda error \n",
        "\n",
        "# for defining network\n",
        "fv = [256, 256, 512, 1024, 2048] #Channels/Convolutions of Images\n",
        "fv_inv = [1024, 512, 256] #Channels/Tranpose Convolutions of Latent Space\n",
        "ks = 3 #kernel size\n",
        "feat_size = 512 #Feature Space Size\n",
        "latent_features = 20 # dimensionality of latent space\n",
        "\n",
        "# for defining loss\n",
        "alpha = 2 #how much to weight MSE loss\n",
        "\n",
        "# for defining training \n",
        "epochs = 10 # number of epochs to train for\n",
        "lr = 1e-6 # learning rate of SGD optimizer\n",
        "w_d = 1e-5 # weight decay of SGD optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dONXS-SEAAh6"
      },
      "outputs": [],
      "source": [
        "int_data_all = spio.loadmat('/content/drive/MyDrive/Thon Group Master Folder/Sreyas/Photonic Crystals/Rockfish Training Data Gen/int_total_sqr_no_struct.mat', squeeze_me=True)\n",
        "# ext_data_all = spio.loadmat('/content/drive/MyDrive/Thon Group Master Folder/Sreyas/Photonic Crystals/Rockfish Training Data Gen/ext_total_sqr_no_struct.mat', squeeze_me=True)\n",
        "wavelengths = int_data_all['lambda']\n",
        "wavelengths = np.delete(wavelengths,np.where(wavelengths==[0.5]))\n",
        "wavelengths = np.delete(wavelengths,np.where(wavelengths==[1]))\n",
        "wavelengths = np.transpose(np.tile(wavelengths,(3,1)))\n",
        "\n",
        "spec_points = np.shape(wavelengths)[0] # number of points in the spectra\n",
        "# print(np.shape(wavelengths))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiUSPvdAFf5N",
        "outputId": "72479eda-a636-483c-fff6-5bad63ed20cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ext', 'int']\n",
            "(22, 256, 256)\n"
          ]
        }
      ],
      "source": [
        "# import data from hdf file\n",
        "fname_mask = '/content/drive/MyDrive/Thon Group Master Folder/Sreyas/Photonic Crystals/Rockfish Training Data Gen/allData.h5'\n",
        "\n",
        "hdf_file = h5py.File(fname_mask, \"r\")\n",
        "\n",
        "# # Print statements for investigating data parameters\n",
        "print(list(hdf_file.keys()))\n",
        "dext_spectra = hdf_file['ext/maskCell'] \n",
        "dint_spectra = hdf_file['int/maskCell']\n",
        "combined = np.concatenate((dext_spectra[:11],dint_spectra[:11]), axis=0)\n",
        "print(np.shape(combined))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "i2L8E7dWSNBv"
      },
      "outputs": [],
      "source": [
        "class ImageDataset(Dataset):\n",
        "  #hf is the hdf5 file object\n",
        "  #datanum is the number of datapoints from EACH set that will be used in the model\n",
        "    def __init__(self,  hf, datanum, transform= transforms.Compose([ transforms.ToTensor(), transforms.ConvertImageDtype(dtype=torch.half)])  ):\n",
        "        super(Dataset, self).__init__()\n",
        "        dext_height = hf['ext/height'] # height values of ext data\n",
        "        dint_height = hf['int/height'] # height values of int data\n",
        "        dext_spectra = hf['ext/spectCell'] # spectra values of ext data\n",
        "        dint_spectra = hf['int/spectCell'] # spectra values of int data\n",
        "\n",
        "        # get data into correct format\n",
        "        self.spectra = torch.from_numpy(np.concatenate((dext_spectra[:datanum], dint_spectra[:datanum]), axis=0)) # spectra\n",
        "        self.heights = torch.from_numpy(np.concatenate((dext_height[:datanum],dint_height[:datanum]), axis=0)) # heights\n",
        "        self.masks = np.concatenate((hf['ext/maskCell'][:datanum],hf['int/maskCell'][:datanum]), axis=0)  # masks\n",
        "        self.sizes =  torch.from_numpy(np.concatenate((hf['ext/size'][:datanum],hf['int/size'][:datanum]), axis=0)) # could switch to sreyas size calculation instead, not sure\n",
        "        ext_label = np.ones((np.shape(dext_height))) # 1 label for ext data\n",
        "        int_label = np.zeros((np.shape(dint_height))) # 0 label for ext data\n",
        "        self.labels = torch.from_numpy(np.concatenate( (ext_label, int_label), axis=0)) # combine labels\n",
        "        self.transform = transform             \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sizes)\n",
        "\n",
        "    # currently extracting image, spectra and height \n",
        "    def __getitem__(self, idx): \n",
        "        image = (self.masks[idx,:,:]) # input mask image\n",
        "        spectra = (( self.spectra[idx] )) # spectra  \n",
        "        if self.transform: \n",
        "            image = self.transform(image) \n",
        "        return image, spectra"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1E9Qyv9qf2zS",
        "outputId": "a372a3fa-0bf8-4319-e90b-cc419206a53b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['absp', 'ext_in', 'height', 'height_bin', 'lambda', 'masks', 'refl', 'size_frac', 'tran']\n",
            "(50000, 256, 256)\n"
          ]
        }
      ],
      "source": [
        "#Testing for Thin H5 \n",
        "fname_mask2 ='/content/drive/MyDrive/Thon Group Master Folder/Sreyas/Photonic Crystals/Rockfish Training Data Gen/allData_thin.h5'\n",
        "hdf_file2 = h5py.File(fname_mask2, \"r\")\n",
        "print(list(hdf_file2.keys()))\n",
        "combined2=np.concatenate((hdf_file2['masks'][:11], hdf_file2['masks'][25000:25000+11]), axis = 0)\n",
        "print(np.shape(hdf_file2['masks']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1UWWAQ5-7r3X"
      },
      "outputs": [],
      "source": [
        "#Different Dataloader for thin data\n",
        "\n",
        "class ImageDataset_thin(Dataset):\n",
        "  #hf is the hdf5 file object\n",
        "  #datanum is the number of datapoints from EACH set that will be used in the model\n",
        "    def __init__(self,  hf, datanum, transform= transforms.Compose([ transforms.ToTensor(), transforms.ConvertImageDtype(dtype=torch.half)])  ):\n",
        "        super(Dataset, self).__init__()\n",
        "\n",
        "        #Format\n",
        "        self.masks = np.concatenate((hf['masks'][:datanum],hf['masks'][25000:25000+datanum]), axis=0)  # masks\n",
        "        self.absp = torch.from_numpy(np.concatenate(hf['absp'], hf['absp'][25000:25000+datanum]), axis=0)\n",
        "        self.refl = torch.from_numpy(np.concatenate(hf['refl'], hf['refl'][25000:25000+datanum]), axis=0)       \n",
        "        self.tran = torch.from_numpy(np.concatenate(hf['tran'], hf['tran'][25000:25000+datanum]), axis=0) # spectra\n",
        "        self.heights = torch.from_numpy(np.concatenate(hf['height'],hf['height'][250001:25000+datanum]), axis=0) # heights\n",
        "        self.height_bin = torch.from_numpy(np.concatenate(hf['height_bin'][:datanum], hf['height_bin'][25000:25000+datanum]), axis=0) # bins\n",
        "        self.size_frac =  torch.from_numpy(np.concatenate(hf['size_frac'][:datanum], hf['size_frac'][25000:25000+datanum]), axis=0) #\n",
        "        self.label = torch.from_numpy(np.concatenate(hf['ext_in'][:datanum], hf['ext_in'][25000:25000+datanum]))\n",
        "        self.transform = transform         \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sizes)\n",
        "\n",
        "    # currently extracting image, spectra and height \n",
        "    def __getitem__(self, idx): \n",
        "        image = self.masks[idx,:,:] # input mask image\n",
        "        absp =  self.absp[:,idx]\n",
        "        refl = self.refl[:,idx]\n",
        "        tran = self.tran[:,idx]\n",
        "        height = self.heights[:,idx]\n",
        "        if self.transform: \n",
        "            image = self.transform(image)\n",
        "             \n",
        "        return image, absp, refl, tran, height"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JQFoFVg9-vW9"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwkLOUFzV73Z",
        "outputId": "50411789-9700-4297-83f2-a2e27519f736"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split Seed is: 42\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# # Data preparation\n",
        "\n",
        "full_dataset = ImageDataset(hdf_file, half_data_num )\n",
        "\n",
        "# Define ratios of train, validation and testing data\n",
        "train_size = int(0.7 * len(full_dataset))\n",
        "val_size = int(0.2 * len(full_dataset))\n",
        "test_size = int(0.1 * len(full_dataset))\n",
        "\n",
        "# Use random split with seed\n",
        "split_seed=42;\n",
        "print(\"Split Seed is:\", split_seed)\n",
        "data_train, data_val, data_test = torch.utils.data.random_split(full_dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(split_seed))\n",
        "\n",
        "\n",
        "# Split data into random batches\n",
        "train_dataloader = DataLoader(data_train, batch_size = bsize, shuffle=True)\n",
        "test_dataloader = DataLoader(data_test, batch_size = bsize)\n",
        "valid_dataloader = DataLoader(data_val, batch_size = bsize)\n",
        "\n",
        "# clear some unnecessary variables\n",
        "#del hdf_file, int_data_all\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7Rw4snXosYY"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        " \n",
        "```\n",
        "\n",
        "# \"Encoder\" (combined Feature Extraction Network, Prediction Network and Recognition Network)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "m2NaHNYXom4C"
      },
      "outputs": [],
      "source": [
        "from re import X\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # Feature Extraction Network\n",
        "    self.enc1 = nn.Sequential( # Feature Extraction Network extraction_layers\n",
        "            # Conv_1\n",
        "            nn.Conv2d(1, fv[0], kernel_size=ks, stride=1, padding=\"same\"),\n",
        "            nn.BatchNorm2d(fv[0]),\n",
        "            nn.LeakyReLU(negative_slope=0.1, inplace=True),\n",
        "            \n",
        "            # Conv_2 + Pool_1\n",
        "            nn.Conv2d(fv[0], fv[1], kernel_size=ks,  stride=1, padding=\"same\"),\n",
        "            nn.BatchNorm2d(fv[1]),\n",
        "            nn.LeakyReLU(negative_slope=0.1, inplace=True),     \n",
        "            nn.MaxPool2d(2,2),\n",
        "\n",
        "            # Conv_3 + Pool_2\n",
        "            nn.Conv2d(fv[1], fv[2], kernel_size=ks,  stride=1, padding=\"same\"),\n",
        "            nn.BatchNorm2d(fv[2]),\n",
        "            nn.LeakyReLU(negative_slope=0.1, inplace=True),\n",
        "            nn.MaxPool2d(2,2),\n",
        "\n",
        "            # Conv_4 + Pool_3\n",
        "            nn.Conv2d(fv[2], fv[3], kernel_size=ks,  stride=1, padding=\"same\"),\n",
        "            nn.BatchNorm2d(fv[3]),\n",
        "            nn.LeakyReLU(negative_slope=0.1, inplace=True),\n",
        "            nn.MaxPool2d(2,2),\n",
        "        )\n",
        "    self.enc2 = nn.Sequential( # feature_layer of Feature Extraction Network\n",
        "            nn.Linear(32*32*1024, feat_size), #reducing to feat_size\n",
        "            nn.BatchNorm2d(1)\n",
        "        )\n",
        "\n",
        "    # # Prediction network\n",
        "    self.pred1 = nn.Sequential( \n",
        "            nn.Linear(feat_size, feat_size),\n",
        "            nn.BatchNorm2d(1),\n",
        "            nn.Linear(feat_size, feat_size),\n",
        "            nn.BatchNorm2d(1),\n",
        "            nn.Linear(feat_size, spec_points*3)\n",
        "        )\n",
        "\n",
        "\n",
        "    # # Recognition network\n",
        "    self.rec1 =  nn.Sequential( nn.Linear(feat_size+3*spec_points, feat_size),\n",
        "                               nn.BatchNorm2d(1),\n",
        "    )\n",
        "    self.fc_mean = nn.Linear(feat_size, latent_features)\n",
        "    self.fc_cov = nn.Linear(feat_size, latent_features)\n",
        "    \n",
        "  def forward(self, x):\n",
        "\n",
        "    # # Run Feature Extraction Network\n",
        "    # encoding\n",
        "    # batch size usually equals int(torch.numel(x)/256/256), but the final batch in loader may be less than batch size\n",
        "    x = (x.view(int(torch.numel(x)/256/256), 1, 256, 256)) # get x in format of 4D tensor for inputting into Conv layers \n",
        "    x = self.enc1(x)\n",
        "\n",
        "    # get e in format of 4D tensor for inputting into Linear layer \n",
        "    e = x.view(int(torch.numel(x)/32/32/1024), 32*32*1024)     \n",
        "    e = torch.unsqueeze(e,1)\n",
        "    e = torch.unsqueeze(e,1)    \n",
        "    e = self.enc2(e) # run linear layer\n",
        "\n",
        "    # # Run Prediction Network\n",
        "    p = torch.sigmoid(self.pred1(e)) # p = predicted spectra\n",
        "\n",
        "    # # Run Recognition Network\n",
        "    input_rec = torch.cat((e,p), 3) # Combine condensed geometry features with the predicted spectra    \n",
        "    x = self.rec1(input_rec)\n",
        "    mu = self.fc_mean(x)\n",
        "    log_var = self.fc_cov(x)\n",
        "\n",
        "    return p, mu, log_var "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yfuli95YPBvJ"
      },
      "source": [
        "# \"Decoder\" (Regeneration Network)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "qb-IS3NdpzEO"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.recog = nn.Sequential(\n",
        "          #Fc_4 Fc_5 Fc_6\n",
        "          nn.Linear(latent_features+3*spec_points, feat_size),\n",
        "          nn.BatchNorm2d(1),\n",
        "          nn.Linear(feat_size, feat_size),\n",
        "          nn.BatchNorm2d(1),\n",
        "          nn.Linear(feat_size, 32*32*fv_inv[0]),\n",
        "          nn.BatchNorm2d(1),\n",
        "        )\n",
        "\n",
        "    self.reconstruct = nn.Sequential(\n",
        "          nn.ConvTranspose2d(fv_inv[0], fv_inv[1], 3, stride=2, padding = 1, output_padding=1),\n",
        "          nn.ConvTranspose2d(fv_inv[1], fv_inv[2], 3, stride=2, padding = 1, output_padding=1),\n",
        "          nn.ConvTranspose2d(fv_inv[2], 1, 3, stride=2, padding = 1, output_padding=1)\n",
        "        )\n",
        "\n",
        "\n",
        "  def forward(self, spectra,latent):\n",
        "    input_dec = torch.cat((spectra,latent),3) # combine spectra output with latent space output\n",
        "    x = self.recog(input_dec) # run recognition layer\n",
        "    x = x.view(int(torch.numel(x)/32/32/fv_inv[0]), fv_inv[0], 32, 32) # ensure the 4D tensor is the correct size\n",
        "    x = self.reconstruct(x) # run reconstruction layer\n",
        "    reconstruction =  torch.sigmoid( x )\n",
        "    return reconstruction # output geometry based on spectra and latent space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYVR3mSaQPua"
      },
      "source": [
        "# VAE Model Definition (includes reparameterization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "WJKSI3gy9t3Z"
      },
      "outputs": [],
      "source": [
        "# primary VAE module\n",
        "\n",
        "class CustomVAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomVAE, self).__init__()\n",
        "        self.encoder = Encoder()\n",
        "        self.decoder = Decoder()\n",
        "\n",
        "    def reparameterize(self, mu, log_var):\n",
        "        \"\"\"\n",
        "        :param mu: mean from the encoder's latent space\n",
        "        :param log_var: log variance from the encoder's latent space\n",
        "        \"\"\"\n",
        "        std = torch.exp(0.5*log_var) # standard deviation\n",
        "        eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
        "        sample = mu + (eps * std) # sampling as if coming from the input space\n",
        "        return sample\n",
        "\n",
        "    def forward(self, x):      \n",
        "        x = self.encoder(x)  \n",
        "        spectra, mu, log_var  = x        \n",
        "\n",
        "        # get the latent vector through reparameterization\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "        recon_x = self.decoder( spectra, z )\n",
        "        recon_x = recon_x.view(int(torch.numel(recon_x)/256/256), 256, 256) # reconstructed geometry\n",
        "        return recon_x, mu, log_var, spectra\n",
        "\n",
        "    # TO DO: DEFINE SPECTRA PREDICTION FUNCTION AND GEOMETRY GENERATION\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MR7Zym-zQcF3"
      },
      "source": [
        ":# Training and Validation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Ag-qT_d6DluX"
      },
      "outputs": [],
      "source": [
        "# combined loss function that determines the entire network\n",
        "def final_loss(loss1_bce, loss2_mse, mu, logvar):\n",
        "    \"\"\"\n",
        "    This function will add the reconstruction loss and the  KL-Divergence.\n",
        "    KL-Divergence = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
        "    :param mu: the mean from the latent vector\n",
        "    :param logvar: log variance from the latent vector\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    # KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) # previous KLD definition\n",
        "    KLD = 0\n",
        "    # print(KLD)\n",
        "    return (loss1_bce + alpha*loss2_mse + KLD)\n",
        "\n",
        "    # TO DO: determine whether KLD is still going to NAN or not\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "MYLRiGIgMw0a"
      },
      "outputs": [],
      "source": [
        "# fit function for training the model\n",
        "def fit(model, dataloader):\n",
        "    model.train() \n",
        "    running_loss = 0.0\n",
        "    for i, data in tqdm(enumerate(dataloader), total=int(len(train_dataloader)/dataloader.batch_size)):\n",
        "      if torch.cuda.is_available():\n",
        "        # with autograd.detect_anomaly(): # uncomment this to debug when you receive \"Cuda: device-side assert error\"  \n",
        "          \n",
        "          data, spectra_in = [d.cuda( ) for d in data] # load in data from data loader\n",
        "          data = data.view(int(torch.numel(data)/256/256),  256, 256) # resize data for loss function later\n",
        "          optimizer.zero_grad() # initialize gradients to zero\n",
        "\n",
        "          reconstruction, mu, logvar, out_spectra = model(data) # run model\n",
        "          pout = out_spectra.view(-1, 3, spec_points)  # reformat spectra for plotting\n",
        "          \n",
        "          # leftover code from when i was trying to debug NAN error, potentially unnecessary\n",
        "          reconstruction = reconstruction.clamp(0,1) # clamp between 0 and 1\n",
        "          reconstruction[reconstruction!=reconstruction] = 1 # set NAN values  \n",
        "\n",
        "          # solve for loss\n",
        "          bce_loss = criterion_mask(reconstruction, data)          \n",
        "          mse_loss = criterion(spectra_in.float(), pout)          \n",
        "          loss = final_loss(bce_loss, mse_loss, mu, logvar)\n",
        "          running_loss += loss.item()          \n",
        "\n",
        "          # backpropagate loss and then step the optimizer\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "    train_loss = running_loss/len(dataloader.dataset)\n",
        "    return train_loss\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "tW0UOB4UGbxz"
      },
      "outputs": [],
      "source": [
        "# validate function for testing the model on the validation data\n",
        "\n",
        "def validate(model, dataloader, plot_on):\n",
        "  model.eval()\n",
        "  running_loss = 0.0\n",
        "  with torch.no_grad(): # all gradients off because currently just evaluating the model\n",
        "    for i, data in tqdm(enumerate(dataloader), total=int(len(valid_dataloader)/dataloader.batch_size)):\n",
        "        data, spectra_in = [d.cuda( ) for d in data] # load in data from data loader\n",
        "        len_temp = int(torch.numel(data)/256/256) # usually this is batch size, but sometimes last batch is a different size\n",
        "        data = data.view(len_temp,  256, 256) # resize data for loss function later\n",
        "\n",
        "        reconstruction, mu, logvar, out_spectra = model(data) # run model\n",
        "        pout = out_spectra.view(-1, 3, spec_points) #reformat spectra for plotting\n",
        "\n",
        "        # leftover code from when i was trying to debug NAN error, potentially unnecessary\n",
        "        reconstruction = reconstruction.clamp(0,1) # clamp between 0 and 1\n",
        "        reconstruction[reconstruction!=reconstruction] = 1 # set NAN values  \n",
        "        \n",
        "        # solve for loss\n",
        "        bce_loss = criterion_mask(reconstruction, data)\n",
        "        mse_loss = criterion(spectra_in.float(), pout)\n",
        "        loss = final_loss(bce_loss, mse_loss, mu, logvar)\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # save and plot the last batch input and output \n",
        "        if plot_on:\n",
        "          if i == len_temp - 1:\n",
        "            print(\"MSE loss: \" + str(mse_loss)) # spectra prediction loss\n",
        "\n",
        "            # first plot shows input geometry vs output geometry\n",
        "            num_replicas = 4\n",
        "            fig, axs = plt.subplots(2,num_replicas)\n",
        "            for x in range( num_replicas ):      \n",
        "              axs[0,x].imshow(torch.squeeze(data.view(len_temp, 1, 256, 256)[x]).cpu())\n",
        "              axs[0,x].xaxis.set_visible(False)\n",
        "              axs[0,x].yaxis.set_visible(False)\n",
        "              axs[1,x].imshow(torch.squeeze(reconstruction.view(len_temp, 1, 256, 256)[x]).cpu())\n",
        "              axs[1,x].xaxis.set_visible(False)\n",
        "              axs[1,x].yaxis.set_visible(False)\n",
        "            fig.suptitle(str(epoch+1))\n",
        "\n",
        "            # second plot shows input spectra vs output spectra\n",
        "            og = spectra_in[0].detach().cpu().numpy()\n",
        "            pred = pout[0].detach().cpu().numpy()\n",
        "            \n",
        "            fig2, axs = plt.subplots(2,1)\n",
        "            axs[0].plot(wavelengths, np.transpose(og) )\n",
        "            axs[1].plot(wavelengths, np.transpose(pred) )\n",
        "            fig2.suptitle(str(epoch+1))\n",
        "\n",
        "            # save figures\n",
        "            fig.savefig(f\"/content/drive/MyDrive/Thon Group Master Folder/Serene/Spectral Selectivity Project/outputs/{epoch+1}geom_output.png\")\n",
        "            fig2.savefig(f\"/content/drive/MyDrive/Thon Group Master Folder/Serene/Spectral Selectivity Project/outputs/{epoch+1}spectra_output.png\")\n",
        "            plt.show()\n",
        "            \n",
        "    val_loss = running_loss/len(dataloader.dataset)\n",
        "    return val_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dn7-DhoFTDNE"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "9azelGQ_RpvB",
        "outputId": "70ef2891-54cf-42e8-da79-6f92271b40fa"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-fa5499b92608>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_custom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomVAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# send model architecture to cuda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_custom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbsize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# print model summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_custom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw_d\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#use SGD optimizer to mimic paper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcriterion_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sum'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# use BCE loss for mask / geometry criterion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# use MSE loss for spectra criterion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \"\"\"\n\u001b[0;32m--> 680\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mxpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m         \"\"\"\n\u001b[0;32m--> 680\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mxpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model_custom = CustomVAE().cuda() # send model architecture to cuda\n",
        "print(summary(model_custom, input_size = (bsize,1,256,256))) # print model summary\n",
        "optimizer = optim.SGD(model_custom.parameters(), lr=lr, weight_decay = w_d) #use SGD optimizer to mimic paper\n",
        "criterion_mask = nn.BCELoss(reduction='sum') # use BCE loss for mask / geometry criterion\n",
        "criterion = nn.MSELoss() # use MSE loss for spectra criterion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltNHCfWckbEp"
      },
      "outputs": [],
      "source": [
        "# check that cuda is at acceptable limits\n",
        "torch.cuda.synchronize()\n",
        "torch.cuda.empty_cache()\n",
        "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhnrQZhnVF4t"
      },
      "outputs": [],
      "source": [
        "# Loop over epochs \n",
        "train_loss = []\n",
        "val_loss = []\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1} of {epochs}\")\n",
        "    \n",
        "    # empty cuda cache to help prevent unneeded memory usage\n",
        "    torch.cuda.synchronize()\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    # fit data\n",
        "    train_epoch_loss = fit(model_custom, train_dataloader) \n",
        "\n",
        "    # test on validation data\n",
        "    if epoch == 0 or not ((epoch+1) % 5): # plot output every 5 epochs\n",
        "      val_epoch_loss = validate(model_custom, valid_dataloader, 1)\n",
        "    else: # determine validation loss without plotting\n",
        "      val_epoch_loss = validate(model_custom, valid_dataloader, 0)\n",
        "    \n",
        "    # add to variables\n",
        "    train_loss.append(train_epoch_loss)\n",
        "    val_loss.append(val_epoch_loss)\n",
        "\n",
        "    # print progress\n",
        "    print(f\"Train Loss: {train_epoch_loss:.4f}\")\n",
        "    print(f\"Val Loss: {val_epoch_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjHbx9VYvYtB"
      },
      "outputs": [],
      "source": [
        "# print post summary of GPU usage\n",
        "!nvidia-smi"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "AE-framework-draft.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}